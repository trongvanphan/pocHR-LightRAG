# üìö H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng LightRAG

## üéØ Gi·ªõi Thi·ªáu

**LightRAG** l√† h·ªá th·ªëng **Retrieval-Augmented Generation (RAG)** d·ª±a tr√™n Knowledge Graph, ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi ƒë·ªôi ng≈© HKUDS. H·ªá th·ªëng n√†y k·∫øt h·ª£p ƒë·ªì th·ªã tri th·ª©c (Knowledge Graph) v·ªõi vector embeddings ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa vi·ªác t√¨m ki·∫øm v√† tr·∫£ l·ªùi c√¢u h·ªèi.

### ‚ú® T√≠nh NƒÉng Ch√≠nh
- **Knowledge Graph RAG**: X√¢y d·ª±ng ƒë·ªì th·ªã tri th·ª©c t·ª´ t√†i li·ªáu
- **Web UI tr·ª±c quan**: Giao di·ªán web ƒë·ªÉ qu·∫£n l√Ω t√†i li·ªáu v√† truy v·∫•n
- **API t∆∞∆°ng th√≠ch Ollama**: T√≠ch h·ª£p d·ªÖ d√†ng v·ªõi c√°c chatbot AI
- **ƒêa d·∫°ng LLM**: H·ªó tr·ª£ OpenAI, Ollama, Azure OpenAI, Gemini, AWS Bedrock
- **Nhi·ªÅu l·ª±a ch·ªçn l∆∞u tr·ªØ**: PostgreSQL, Neo4j, MongoDB, Milvus, Redis, Qdrant

---

## ‚ö° C√†i ƒê·∫∑t Nhanh

### Y√™u C·∫ßu H·ªá Th·ªëng
- **Python**: >= 3.10
- **Package Manager**: [uv](https://docs.astral.sh/uv/) (khuy·∫øn ngh·ªã) ho·∫∑c pip
- **RAM**: Khuy·∫øn ngh·ªã >= 8GB
- **LLM**: C·∫ßn c√≥ API key (OpenAI, Ollama, Gemini, v.v.)

### B∆∞·ªõc 1: Clone Repository
```bash
git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
```

### B∆∞·ªõc 2: C√†i ƒê·∫∑t Dependencies
```bash
# S·ª≠ d·ª•ng uv (khuy·∫øn ngh·ªã)
uv sync --extra api

# K√≠ch ho·∫°t virtual environment
source .venv/bin/activate  # Linux/macOS
# ho·∫∑c: .venv\Scripts\activate  # Windows
```

### B∆∞·ªõc 3: C·∫•u H√¨nh Environment
```bash
# Copy file c·∫•u h√¨nh m·∫´u
cp env.example .env

# Ch·ªânh s·ª≠a file .env v·ªõi th√¥ng tin LLM c·ªßa b·∫°n
nano .env  # ho·∫∑c s·ª≠ d·ª•ng editor kh√°c
```

---

## ‚öôÔ∏è C·∫•u H√¨nh Quan Tr·ªçng (File .env)

### C·∫•u H√¨nh LLM (B·∫Øt Bu·ªôc)

#### S·ª≠ D·ª•ng OpenAI:
```env
LLM_BINDING=openai
LLM_MODEL=gpt-4o
LLM_BINDING_HOST=https://api.openai.com/v1
LLM_BINDING_API_KEY=sk-your-api-key-here

EMBEDDING_BINDING=openai
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_DIM=3072
EMBEDDING_BINDING_HOST=https://api.openai.com/v1
EMBEDDING_BINDING_API_KEY=sk-your-api-key-here
```

#### S·ª≠ D·ª•ng Ollama (Ch·∫°y Local):
```env
LLM_BINDING=ollama
LLM_MODEL=qwen2.5:32b
LLM_BINDING_HOST=http://localhost:11434

EMBEDDING_BINDING=ollama
EMBEDDING_MODEL=bge-m3:latest
EMBEDDING_DIM=1024
EMBEDDING_BINDING_HOST=http://localhost:11434
```

> ‚ö†Ô∏è **L∆∞u √Ω quan tr·ªçng v·ªõi Ollama**: Context size ph·∫£i >= 32k tokens
> ```bash
> # T·∫°o model v·ªõi context size l·ªõn h∆°n
> ollama show --modelfile qwen2.5:32b > Modelfile
> echo "PARAMETER num_ctx 32768" >> Modelfile
> ollama create -f Modelfile qwen2.5-32k
> ```

#### S·ª≠ D·ª•ng Google Gemini:
```env
LLM_BINDING=gemini
LLM_MODEL=gemini-flash-latest
LLM_BINDING_API_KEY=your-gemini-api-key
LLM_BINDING_HOST=https://generativelanguage.googleapis.com

EMBEDDING_BINDING=gemini
EMBEDDING_MODEL=gemini-embedding-001
EMBEDDING_DIM=1536
EMBEDDING_BINDING_API_KEY=your-gemini-api-key
EMBEDDING_BINDING_HOST=https://generativelanguage.googleapis.com
```

### C·∫•u H√¨nh Server (T√πy Ch·ªçn)
```env
HOST=0.0.0.0
PORT=9621
WEBUI_TITLE='My Knowledge Base'
WEBUI_DESCRIPTION="LightRAG Knowledge Graph System"
```

---

## üöÄ Ch·∫°y ·ª®ng D·ª•ng

### C√°ch 1: Ch·∫°y Server Tr·ª±c Ti·∫øp
```bash
# K√≠ch ho·∫°t virtual environment
source .venv/bin/activate

# Ch·∫°y server
lightrag-server
```
**Truy c·∫≠p**: http://localhost:9621

### C√°ch 2: Ch·∫°y V·ªõi Docker Compose
```bash
# Ch·ªânh s·ª≠a .env tr∆∞·ªõc
docker compose up
```
**Truy c·∫≠p**: http://localhost:9621

### C√°ch 3: Ch·∫°y V·ªõi Gunicorn (Production)
```bash
lightrag-gunicorn
```

---

## üåê S·ª≠ D·ª•ng Web UI

Sau khi server ch·∫°y, m·ªü tr√¨nh duy·ªát v√† truy c·∫≠p: `http://localhost:9621`

### üìÅ Tab Documents (T√†i Li·ªáu)
1. **Upload t√†i li·ªáu**: H·ªó tr·ª£ TXT, PDF, DOCX, PPTX, CSV
2. **Xem tr·∫°ng th√°i x·ª≠ l√Ω**: Theo d√µi ti·∫øn ƒë·ªô indexing
3. **Qu·∫£n l√Ω t√†i li·ªáu**: X√≥a, c·∫≠p nh·∫≠t

### üîç Tab Retrieval (Truy V·∫•n)

#### Query Mode (Ch·∫ø ƒë·ªô truy v·∫•n)

| Mode | M√¥ t·∫£ | Khi n√†o d√πng |
|------|-------|--------------|
| **Naive** | T√¨m ki·∫øm vector truy·ªÅn th·ªëng tr√™n text chunks | C√¢u h·ªèi ƒë∆°n gi·∫£n, t√¨m ki·∫øm t·ª´ kh√≥a |
| **Local** | T·∫≠p trung v√†o **entity** (th·ª±c th·ªÉ) trong Knowledge Graph | H·ªèi v·ªÅ m·ªôt ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ |
| **Global** | T·∫≠p trung v√†o **relationships** (quan h·ªá) trong KG | H·ªèi v·ªÅ m·ªëi li√™n h·ªá gi·ªØa c√°c ƒë·ªëi t∆∞·ª£ng |
| **Hybrid** | Local + Global | C√¢u h·ªèi ph·ª©c t·∫°p c·∫ßn c·∫£ entity v√† relationship |
| **Mix** | Local + Global + Naive | **Khuy·∫øn ngh·ªã** - K·∫øt h·ª£p t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p |
| **Bypass** | B·ªè qua retrieval, g·ª≠i th·∫≥ng c√¢u h·ªèi t·ªõi LLM | Chat th∆∞·ªùng, kh√¥ng c·∫ßn context t·ª´ t√†i li·ªáu |

#### Response Format (ƒê·ªãnh d·∫°ng ph·∫£n h·ªìi)

| Format | M√¥ t·∫£ |
|--------|-------|
| **Multiple Paragraphs** | C√¢u tr·∫£ l·ªùi d√†i, nhi·ªÅu ƒëo·∫°n vƒÉn |
| **Single Paragraph** | C√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn, 1 ƒëo·∫°n |
| **Bullet Points** | Danh s√°ch g·∫°ch ƒë·∫ßu d√≤ng |

#### Token Parameters (Gi·ªõi h·∫°n token)

| Parameter | Default | M√¥ t·∫£ |
|-----------|---------|-------|
| **KG Top K** | 40 | S·ªë l∆∞·ª£ng entities/relations l·∫•y t·ª´ Knowledge Graph. √Åp d·ª•ng cho t·∫•t c·∫£ mode tr·ª´ Naive |
| **Chunk Top K** | 20 | S·ªë l∆∞·ª£ng text chunks l·∫•y t·ª´ vector search. √Åp d·ª•ng cho **t·∫•t c·∫£** modes |
| **Max Entity Tokens** | 6000 | Gi·ªõi h·∫°n tokens cho context v·ªÅ entities |
| **Max Relation Tokens** | 8000 | Gi·ªõi h·∫°n tokens cho context v·ªÅ relationships |
| **Max Total Tokens** | 30000 | T·ªïng budget tokens cho to√†n b·ªô query context |

#### Advanced Options (T√πy ch·ªçn n√¢ng cao)

| Option | Default | M√¥ t·∫£ |
|--------|---------|-------|
| **History Turns** | 0 | S·ªë l∆∞·ª£ng c·∫∑p h·ªôi tho·∫°i (user-assistant) ƒë·ªÉ gi·ªØ context. 0 = kh√¥ng gi·ªØ l·ªãch s·ª≠ |
| **Only Need Context** | Off | B·∫≠t = ch·ªâ tr·∫£ v·ªÅ context retrieved, kh√¥ng generate response |
| **Only Need Prompt** | Off | B·∫≠t = ch·ªâ tr·∫£ v·ªÅ prompt, kh√¥ng generate response |
| **Stream Response** | On | B·∫≠t = streaming real-time response |
| **Enable Rerank** | On | B·∫≠t = s·∫Øp x·∫øp l·∫°i k·∫øt qu·∫£ (c·∫ßn c·∫•u h√¨nh reranker model) |
| **Additional Output Prompt** | Empty | Prompt b·ªï sung cho LLM v·ªÅ c√°ch format output |

#### Khuy·∫øn Ngh·ªã C·∫•u H√¨nh

**Cho c√¢u h·ªèi ƒë∆°n gi·∫£n:**
- Mode: `Naive` ho·∫∑c `Local`
- KG Top K: 20
- Chunk Top K: 10

**Cho c√¢u h·ªèi ph·ª©c t·∫°p:**
- Mode: `Mix` (recommended)
- KG Top K: 40-60
- Chunk Top K: 20-30

**Cho summarization:**
- Mode: `Global`
- Response Format: Multiple Paragraphs
- Max Total Tokens: 50000+

### üï∏Ô∏è Tab Graph (ƒê·ªì Th·ªã)
- Xem tr·ª±c quan Knowledge Graph
- ƒêi·ªÅu h∆∞·ªõng gi·ªØa c√°c entities v√† relationships
- Zoom v√† pan ƒë·ªÉ kh√°m ph√° ƒë·ªì th·ªã

---

## üíª S·ª≠ D·ª•ng API

### Query API
```bash
curl -X POST http://localhost:9621/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "C√¢u h·ªèi c·ªßa b·∫°n ·ªü ƒë√¢y",
    "mode": "hybrid"
  }'
```

### Insert Document API
```bash
curl -X POST http://localhost:9621/documents/text \
  -H "Content-Type: application/json" \
  -d '{
    "text": "N·ªôi dung t√†i li·ªáu c·ªßa b·∫°n..."
  }'
```

### Upload File API
```bash
curl -X POST http://localhost:9621/documents/upload \
  -F "file=@/path/to/your/document.pdf"
```

---

## üêç S·ª≠ D·ª•ng V·ªõi Python Code

```python
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed

WORKING_DIR = "./rag_storage"

async def main():
    # Kh·ªüi t·∫°o LightRAG
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
    )
    
    # B·∫ÆT BU·ªòC: Kh·ªüi t·∫°o storage
    await rag.initialize_storages()
    
    try:
        # Th√™m t√†i li·ªáu
        await rag.ainsert("N·ªôi dung t√†i li·ªáu c·ªßa b·∫°n ·ªü ƒë√¢y...")
        
        # Truy v·∫•n
        result = await rag.aquery(
            "C√¢u h·ªèi c·ªßa b·∫°n?",
            param=QueryParam(mode="hybrid")
        )
        print(result)
        
    finally:
        # ƒê√≥ng k·∫øt n·ªëi
        await rag.finalize_storages()

if __name__ == "__main__":
    asyncio.run(main())
```

---

## üìä L·ª±a Ch·ªçn Model Ph√π H·ª£p

### LLM (Large Language Model)
| Lo·∫°i | Model Khuy·∫øn Ngh·ªã | Ghi Ch√∫ |
|------|-------------------|---------|
| **T·ªët nh·∫•t** | GPT-4o, Claude 3.5 | Ch√≠nh x√°c cao, t·ªën chi ph√≠ |
| **C√¢n b·∫±ng** | GPT-4o-mini, Gemini Flash | T·ªët cho h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p |
| **Local** | Qwen2.5 32B, Llama 3.1 70B | C·∫ßn GPU m·∫°nh |

### Embedding Model
| Model | Dimension | Ghi Ch√∫ |
|-------|-----------|---------|
| `text-embedding-3-large` | 3072 | T·ªët nh·∫•t (OpenAI) |
| `BAAI/bge-m3` | 1024 | ƒêa ng√¥n ng·ªØ, mi·ªÖn ph√≠ |
| `jina-embeddings-v4` | 2048 | ƒêa ng√¥n ng·ªØ |

### Reranker (T√πy Ch·ªçn)
Th√™m v√†o `.env` ƒë·ªÉ c·∫£i thi·ªán k·∫øt qu·∫£:
```env
RERANK_BINDING=cohere
RERANK_MODEL=rerank-v3.5
RERANK_BINDING_API_KEY=your-cohere-api-key
```

---

## üîß C√°c L·ªánh H·ªØu √çch

```bash
# Xem help c·ªßa server
lightrag-server --help

# Xem options cho LLM binding c·ª• th·ªÉ
lightrag-server --llm-binding openai --help
lightrag-server --llm-binding ollama --help

# T·∫£i cache offline
lightrag-download-cache

# D·ªçn d·∫πp cache LLM query
lightrag-clean-llmqc
```

---

## üêõ X·ª≠ L√Ω L·ªói Th∆∞·ªùng G·∫∑p

### L·ªói 1: Connection refused v·ªõi Ollama
**Nguy√™n nh√¢n**: Ollama server ch∆∞a ch·∫°y
**Gi·∫£i ph√°p**:
```bash
ollama serve
```

### L·ªói 2: Context length exceeded
**Nguy√™n nh√¢n**: Context size c·ªßa model qu√° nh·ªè
**Gi·∫£i ph√°p**: Th√™m v√†o `.env`:
```env
OLLAMA_LLM_NUM_CTX=32768
```

### L·ªói 3: Embedding dimension mismatch
**Nguy√™n nh√¢n**: ƒê·ªïi embedding model sau khi ƒë√£ index t√†i li·ªáu
**Gi·∫£i ph√°p**: X√≥a th∆∞ m·ª•c `rag_storage` v√† index l·∫°i:
```bash
rm -rf ./rag_storage
# ho·∫∑c gi·ªØ cache LLM:
# gi·ªØ l·∫°i file kv_store_llm_response_cache.json
```

### L·ªói 4: Rate limit exceeded (OpenAI)
**Gi·∫£i ph√°p**: Gi·∫£m concurrent requests trong `.env`:
```env
MAX_ASYNC=2
EMBEDDING_FUNC_MAX_ASYNC=4
```

---

## üìÅ C·∫•u Tr√∫c Th∆∞ M·ª•c

```
pocHR-LightRAG/
‚îú‚îÄ‚îÄ .env                    # File c·∫•u h√¨nh (t·∫°o t·ª´ env.example)
‚îú‚îÄ‚îÄ .venv/                  # Virtual environment
‚îú‚îÄ‚îÄ lightrag/               # Source code ch√≠nh
‚îÇ   ‚îú‚îÄ‚îÄ api/                # REST API server
‚îÇ   ‚îú‚îÄ‚îÄ kg/                 # Knowledge Graph storage
‚îÇ   ‚îú‚îÄ‚îÄ llm/                # LLM integrations
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ lightrag_webui/         # Frontend Web UI
‚îú‚îÄ‚îÄ examples/               # V√≠ d·ª• code
‚îú‚îÄ‚îÄ docs/                   # T√†i li·ªáu chi ti·∫øt
‚îú‚îÄ‚îÄ rag_storage/            # D·ªØ li·ªáu knowledge graph (t·ª± t·∫°o)
‚îî‚îÄ‚îÄ inputs/                 # Th∆∞ m·ª•c upload t√†i li·ªáu (t·ª± t·∫°o)
```

---

## üë• HR CV Management Module

Module HR cho ph√©p qu·∫£n l√Ω CV ·ª©ng vi√™n, ph√¢n t√≠ch v√† matching v·ªõi job description.

### T√≠nh NƒÉng

1. **Upload v√† Parse CV**: H·ªó tr·ª£ PDF, DOCX (s·ª≠ d·ª•ng Microsoft MarkItDown)
2. **Extract th√¥ng tin**: LLM t·ª± ƒë·ªông tr√≠ch xu·∫•t th√¥ng tin t·ª´ CV (Vietnamese/English)
3. **Interview Evaluation**: ƒê√°nh gi√° ph·ªèng v·∫•n senior (tr·ªçng s·ªë 2.5x)
4. **Skill Search**: T√¨m ki·∫øm ·ª©ng vi√™n theo k·ªπ nƒÉng (hybrid: KG + vector)
5. **Job Matching**: AI matching ·ª©ng vi√™n v·ªõi job description

### C·∫•u H√¨nh cho HR Module

Th√™m v√†o file `.env`:

```env
# LLM cho HR (khuy·∫øn ngh·ªã Ollama local)
LLM_BINDING=ollama
LLM_MODEL=qwen2.5:7b
LLM_BINDING_HOST=http://localhost:11434
OLLAMA_LLM_NUM_CTX=32768

# Embedding (khuy·∫øn ngh·ªã local)
EMBEDDING_BINDING=ollama
EMBEDDING_MODEL=bge-m3:latest
EMBEDDING_DIM=1024
EMBEDDING_BINDING_HOST=http://localhost:11434

# HR Entity Types
ENTITY_TYPES='["Candidate", "Skill", "Company", "Education", "Certification", "JobPosition", "InterviewEvaluation", "Person", "Organization"]'

# Ng√¥n ng·ªØ h·ªó tr·ª£
SUMMARY_LANGUAGE=Vietnamese,English
```

### Setup Ollama Models

```bash
# Pull embedding model
ollama pull bge-m3:latest

# Pull LLM model
ollama pull qwen2.5:7b
```

### S·ª≠ D·ª•ng trong WebUI

1. **Truy c·∫≠p tab "HR"** trong giao di·ªán web
2. **Upload CV**: Click "Upload CV" v√† ch·ªçn file PDF/DOCX
3. **Xem chi ti·∫øt**: Click v√†o card ·ª©ng vi√™n ƒë·ªÉ xem th√¥ng tin ƒë·∫ßy ƒë·ªß
4. **Th√™m ƒë√°nh gi√°**: Trong tab Evaluations, click "Add Evaluation"
5. **T√¨m ki·∫øm**: Tab "Skill Search" ƒë·ªÉ t√¨m ·ª©ng vi√™n theo k·ªπ nƒÉng
6. **Job Matching**: Tab "Job Matcher" ƒë·ªÉ matching v·ªõi job description

### API Endpoints

| Endpoint | Method | M√¥ t·∫£ |
|----------|--------|-------|
| `/hr/candidates/upload` | POST | Upload CV (multipart/form-data) |
| `/hr/candidates` | GET | Danh s√°ch ·ª©ng vi√™n |
| `/hr/candidates/{id}` | GET | Chi ti·∫øt ·ª©ng vi√™n |
| `/hr/candidates/{id}` | PUT | C·∫≠p nh·∫≠t th√¥ng tin ·ª©ng vi√™n |
| `/hr/candidates/{id}` | DELETE | X√≥a ·ª©ng vi√™n |
| `/hr/candidates/{id}/evaluation` | POST | Th√™m ƒë√°nh gi√° ph·ªèng v·∫•n |
| `/hr/candidates/{id}/skills` | POST | Th√™m skills m·ªõi cho ·ª©ng vi√™n |
| `/hr/skills/search?skill=Python` | GET | T√¨m theo skill |
| `/hr/jobs/match` | POST | Match job description |
| `/hr/skills` | GET | Danh s√°ch t·∫•t c·∫£ skills |

### V√≠ D·ª• API

```bash
# Upload CV
curl -X POST "http://localhost:9621/hr/candidates/upload" \
  -H "Authorization: Bearer <token>" \
  -F "file=@cv.pdf"

# T√¨m theo skill
curl "http://localhost:9621/hr/skills/search?skill=Python&top_k=10"

# Match job
curl -X POST "http://localhost:9621/hr/jobs/match" \
  -H "Content-Type: application/json" \
  -d '{
    "job_description": "Senior Python Developer v·ªõi 5 nƒÉm kinh nghi·ªám...",
    "top_k": 15
  }'
```

### Tr·ªçng S·ªë ƒê√°nh Gi√°

> **Quan tr·ªçng:** ƒê√°nh gi√° ph·ªèng v·∫•n t·ª´ senior c√≥ tr·ªçng s·ªë **2.5x** so v·ªõi th√¥ng tin CV.
> ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o ƒë√°nh gi√° th·ª±c t·∫ø override c√°c claims t·ª´ CV.

---

## üìñ T√†i Li·ªáu Tham Kh·∫£o

- [README ch√≠nh th·ª©c](./README.md)
- [Docker Deployment](./docs/DockerDeployment.md)
- [Frontend Build Guide](./docs/FrontendBuildGuide.md)
- [Offline Deployment](./docs/OfflineDeployment.md)
- [Concurrent Explain](./docs/LightRAG_concurrent_explain.md)
- [GitHub Issues](https://github.com/HKUDS/LightRAG/issues)
- [Discord Community](https://discord.gg/yF2MmDJyGJ)

---

## üéØ Tips S·ª≠ D·ª•ng Hi·ªáu Qu·∫£

1. **Ch·ªçn model ph√π h·ª£p**: V·ªõi t√†i li·ªáu ti·∫øng Vi·ªát, s·ª≠ d·ª•ng embedding model ƒëa ng√¥n ng·ªØ nh∆∞ `bge-m3`

2. **Chunking t·ªëi ∆∞u**: ƒêi·ªÅu ch·ªânh trong `.env`:
   ```env
   CHUNK_SIZE=1200
   CHUNK_OVERLAP_SIZE=100
   ```

3. **Entity types cho ti·∫øng Vi·ªát**:
   ```env
   ENTITY_TYPES='["Ng∆∞·ªùi", "T·ªï Ch·ª©c", "ƒê·ªãa ƒêi·ªÉm", "S·ª± Ki·ªán", "Kh√°i Ni·ªám"]'
   SUMMARY_LANGUAGE=Vietnamese
   ```

4. **Backup d·ªØ li·ªáu**: Th∆∞·ªùng xuy√™n backup th∆∞ m·ª•c `rag_storage/`

---

**Ch√∫c b·∫°n s·ª≠ d·ª•ng LightRAG th√†nh c√¥ng! üöÄ**
